{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-11T04:57:25.971752Z","iopub.status.busy":"2024-03-11T04:57:25.971067Z","iopub.status.idle":"2024-03-11T04:57:40.560003Z","shell.execute_reply":"2024-03-11T04:57:40.559217Z","shell.execute_reply.started":"2024-03-11T04:57:25.971707Z"},"trusted":true},"outputs":[],"source":["\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T04:57:40.562321Z","iopub.status.busy":"2024-03-11T04:57:40.561796Z","iopub.status.idle":"2024-03-11T04:57:40.568450Z","shell.execute_reply":"2024-03-11T04:57:40.567625Z","shell.execute_reply.started":"2024-03-11T04:57:40.562296Z"},"trusted":true},"outputs":[],"source":["\n","BASE_DIR = '/kaggle/input/imagecaption'\n","WORKING_DIR = '/kaggle/working'"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["EXTRACT IMAGE FEATURES"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T04:57:40.569770Z","iopub.status.busy":"2024-03-11T04:57:40.569503Z","iopub.status.idle":"2024-03-11T04:57:46.109561Z","shell.execute_reply":"2024-03-11T04:57:46.108667Z","shell.execute_reply.started":"2024-03-11T04:57:40.569746Z"},"trusted":true},"outputs":[],"source":["\n","model = VGG16()\n","model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n","print(model.summary())"]},{"cell_type":"markdown","metadata":{},"source":["LOADING DATA"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-11T04:57:46.110957Z","iopub.status.busy":"2024-03-11T04:57:46.110680Z"},"trusted":true},"outputs":[],"source":["features = {}\n","directory = os.path.join(BASE_DIR, 'Images')\n","\n","\n","for img_name in tqdm(os.listdir(directory)):\n","    # load the image from file\n","    img_path = directory + '/' + img_name\n","    image = load_img(img_path, target_size=(224, 224))\n","    # convert image pixels to numpy array\n","    image = img_to_array(image)\n","    # reshape data for model\n","    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","    # preprocess image for vgg\n","    image = preprocess_input(image)\n","    # extract features\n","    feature = model.predict(image, verbose=0)\n","    # get image ID\n","    image_id = img_name.split('.')[0]\n","    # store feature\n","    features[image_id] = feature"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","pickle.dump(features, open(os.path.join(WORKING_DIR, 'features.pkl'), 'wb'))"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","with open(os.path.join(WORKING_DIR, 'features.pkl'), 'rb') as f:\n","    features = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["with open(os.path.join(BASE_DIR, 'captions.txt'), 'r') as f:\n","    next(f)\n","    captions_doc = f.read()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","mapping = {}\n","for line in tqdm(captions_doc.split('\\n')):\n","    tokens = line.split(',')\n","    if len(line) < 2:\n","        continue\n","    image_id, caption = tokens[0], tokens[1:]\n","    image_id = image_id.split('.')[0]\n","    caption = \" \".join(caption)\n","    if image_id not in mapping:\n","        mapping[image_id] = []\n","    mapping[image_id].append(caption)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["len(mapping)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def clean(mapping):\n","    for key, captions in mapping.items():\n","        for i in range(len(captions)):\n","            caption = captions[i]\n","            caption = caption.lower() \n","            caption = caption.replace('[^A-Za-z]', '')\n","            caption = caption.replace('\\s+', ' ')\n","            caption = 'startseq ' + \" \".join([word for word in         caption.split() if len(word)>1]) + ' endseq'\n","            captions[i] = caption"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","mapping['1000268201_693b08cb0e']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","clean(mapping)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","mapping['1000268201_693b08cb0e']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["all_captions = []\n","for key in mapping:\n","    for caption in mapping[key]:\n","        all_captions.append(caption)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["len(all_captions)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["all_captions[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(all_captions)\n","vocab_size = len(tokenizer.word_index) + 1"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["vocab_size\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","max_length = max(len(caption.split()) for caption in all_captions)\n","max_length"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["image_ids = list(mapping.keys())\n","split = int(len(image_ids) * 0.90)\n","train = image_ids[:split]\n","test = image_ids[split:]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","def data_generator(data_keys, mapping, features, tokenizer,\n","                   max_length, vocab_size, batch_size):\n","    X1, X2, y = list(), list(), list()\n","    n = 0\n","    while 1:\n","        for key in data_keys:\n","            n += 1\n","            captions = mapping[key]\n","            for caption in captions:\n","                seq = tokenizer.texts_to_sequences([caption])[0]\n","                for i in range(1, len(seq)):\n","                    in_seq, out_seq = seq[:i], seq[i]\n","                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n","                    out_seq = to_categorical([out_seq], \n","                       num_classes=vocab_size)[0]\n","                    X1.append(features[key][0])\n","                    X2.append(in_seq)\n","                    y.append(out_seq)\n","            if n == batch_size:\n","                  X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n","                  yield [X1, X2], y\n","                  X1, X2, y = list(), list(), list()\n","                  n = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","inputs1 = Input(shape=(4096,))\n","fe1 = Dropout(0.4)(inputs1)\n","fe2 = Dense(256, activation='relu')(fe1)\n","inputs2 = Input(shape=(max_length,))\n","se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n","se2 = Dropout(0.4)(se1)\n","se3 = LSTM(256)(se2)\n","decoder1 = add([fe2, se3])\n","decoder2 = Dense(256, activation='relu')(decoder1)\n","outputs = Dense(vocab_size, activation='softmax')(decoder2)\n","\n","model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","plot_model(model, show_shapes=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# train the model\n","epochs = 20\n","batch_size = 32\n","steps = len(train) // batch_size\n","\n","for i in range(epochs):\n","    # create data generator\n","    generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n","    # fit for one epoch\n","    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# save the model\n","model.save(WORKING_DIR+'/best_model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def idx_to_word(integer, tokenizer):\n","    for word, index in tokenizer.word_index.items():\n","        if index == integer:\n","            return word\n","    return None\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def predict_caption(model, image, tokenizer, max_length):\n","    # add start tag for generation process\n","    in_text = 'startseq'\n","    # iterate over the max length of sequence\n","    for i in range(max_length):\n","        # encode input sequence\n","        sequence = tokenizer.texts_to_sequences([in_text])[0]\n","        # pad the sequence\n","        sequence = pad_sequences([sequence], max_length)\n","        # predict next word\n","        yhat = model.predict([image, sequence], verbose=0)\n","        # get index with high probability\n","        yhat = np.argmax(yhat)\n","        # convert index to word\n","        word = idx_to_word(yhat, tokenizer)\n","        # stop if word not found\n","        if word is None:\n","            break\n","        # append word as input for generating next word\n","        in_text += \" \" + word\n","        # stop if we reach end tag\n","        if word == 'endseq':\n","            break\n","    return in_text"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from nltk.translate.bleu_score import corpus_bleu\n","actual, predicted = list(), list()\n","for key in tqdm(test):\n","    captions = mapping[key]\n","    y_pred = predict_caption(model, features[key], tokenizer, max_length)\n","    actual_captions = [caption.split() for caption in captions]\n","    y_pred = y_pred.split()\n","    actual.append(actual_captions)\n","    predicted.append(y_pred)\n","    print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n","    print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from PIL import Image\n","import matplotlib.pyplot as plt\n","def generate_caption(image_name):\n","    # load the image\n","    # image_name = \"1001773457_577c3a7d70.jpg\"\n","    image_id = image_name.split('.')[0]\n","    img_path = os.path.join(BASE_DIR, \"Images\", image_name)\n","    image = Image.open(img_path)\n","    captions = mapping[image_id]\n","    print('---------------------Actual---------------------')\n","    for caption in captions:\n","        print(caption)\n","    # predict the caption\n","    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n","    print('--------------------Predicted--------------------')\n","    print(y_pred)\n","    plt.imshow(image)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["generate_caption(\"1077546505_a4f6c4daa9.jpg\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":3835150,"sourceId":6643563,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
